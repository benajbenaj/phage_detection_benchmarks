---
title: "Chopped Genome Classifications"
output: html_notebook
---

# {.tabset}

## Overview

### Experiment Description

---

#### Genome Chopping

---

Complete genomes were downloaded from RefSeq from the four "kingdoms": archaea, bacteria, fungi, and virus.

These genomes were "chopped" into non-overlapping fragments of the following lengths: 500, 1000, 3000, and 5000 nt.

The program `src/genome_chopper/chopper.py` was used to chop the genomes. Chopped genomes were written to `data/chopped` with 1 file per genome containing all the fragments of a given size for that genome.

The total number of complete genomes that were chopped from each category are as follows:

Kingdom  | No. Chopped Genomes
:-:      | :-:
archaea  | 758
bacteria | 43,576
fungi    | 36
virus    | 22,312

<hr style="border:1px solid gray"> </hr>

#### Fragment selection

---

The following quantity of genome fragments were selected:

Kingdom  | 500 nt | 1000 nt | 3000 nt | 5000 nt
:-:      | :-:    | :-:     | :-:     | :-:
archaea  | 10k    | 10k     | 10k     | 10k
bacteria | 10k    | 10k     | 10k     | 10k
fungi    | 10k    | 10k     | 10k     | 10k
virus    | 10k    | 10k     | 10k     | 10k

Fragment selection was done using the Snakemake pipeline in `src/data_selection/`. Currently, it was performed with replacement. The selected fragments were written to `data/selected_frags/`. Other folders may be added, such as `data/selected_frags_1/`, `data/selected_frags_2/`, *etc.* for replicates.

<hr style="border:1px solid gray"> </hr>

#### Classification

---

The selected genome fragments were classified with the following tools:

* DeepVirFinder
* MetaPhinder
* Seeker
* Unlimited Breadsticks
* VIBRANT
* ViralVerify
* VirFinder
* VirSorter
* VirSorter2

Classification was run using the Snakemake pipeline in `src/classify_chopped/`.

Results were written to `data/classified_chopped/`.

All classifications are combined into the file `data/classified_chopped/combined_out/combined.csv`

This file was cleaned, completed, and had taxonomy added to it with `src/data_analysis/clean_classifications.R`.

<hr style="border:1px solid gray"> </hr>

#### Resource Usage

---

Snakemake benchmark command was used to record resource usage. Individual files are written to `src/classify_chopped/benchmarks/`. I may want to change that and have them written to the `data/classified_chopped_n/` folder(s).

All benchmark files are combined and written to `data/classified_chopped/combined_out/combined_benchmarks.csv`.

<hr style="border:1px solid gray"> </hr>

#### Taxonomy Information

---

Taxonomy information for each fragment was acquired by linking sequence ID -> assembly accession ID -> taxonomic ID -> taxonomy.

The scripts for doing so are in `src/data_analysis`

The workflow was:

`extract_ids.R` -> `get_tax_ids.R` -> `get_taxonomy.R`

The final file containing taxonomy of all genomes that were chopped is `data/refseq_info/taxonomy.csv`

<hr style="border:1px solid gray"> </hr>

### This Analysis

---

All analysis included here is conducted on the combined files:

* `data/classified_chopped/combined_out/cleaned_combined.csv`
* `data/classified_chopped/combined_out/combined_benchmarks.csv`

```{r imports, echo = FALSE}
library(caret)
library(dplyr)
library(forcats)
library(gganimate)
library(ggplot2)
library(ggrepel)     # Line plot text labels
library(gifski)      # Render animations
library(hurwitzLab)  # Colors and palettes
library(stringr)     # str_glue()
library(taxonomizr)
library(tidyr)

# Set ggplot2 theme for whole R session
theme_set(theme_light() +
            theme(plot.title = element_text(hjust = 0.5),
                  plot.subtitle = element_text(hjust = 0.5)))

# Read in all classifications
class_df <-
  read.csv('../../data/classified_chopped/combined_out/cleaned_combined.csv')

# Read in Snakemake benchmarks
raw_bench_df <-
  read.csv('../../data/classified_chopped/combined_out/combined_benchmarks.csv')
```



## Data Cleaning


### Clean Resource Usage

---


```{r fix_bench_names, echo = FALSE}
bench_df <- raw_bench_df %>%
  rename(runtime = s) %>%
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))

bench_df %>% colnames()

bench_df <- bench_df %>%
  mutate(
    tool = case_when(
      tool == "breadsticks" ~ "Unlimited Breadsticks",
      tool == "dvf" ~ "DeepVirFinder",
      tool == "metaphinder" ~ "MetaPhinder",
      tool == "seeker" ~ "Seeker",
      tool == "vibrant" ~ "VIBRANT",
      tool == "viralverify" ~ "viralVerify",
      tool == "virfinder" ~ "VirFinder",
      tool == "virsorter" ~ "VirSorter",
      tool == "virsorter2" ~ "VirSorter2",
    )
  )
```


## Resource Usage

### Recorded Parameters

---

Snakemake benchmark files have the following columns:

Column Name | Unit | Description
:-:         | :-:  | :--
s           | sec  | Running time in seconds
h:m:s       |      | Running time in hour:minute:sec
max_rss     | MB   | Maximum "Resident Set Size”; non-swapped physical memory used
max_vsm     | MB   | Maximum “Virtual Memory Size”; total amount of virtual memory used
max:uss     | MB   | “Unique Set Size”; memory which is unique to a process and which would be freed if the process was terminated right now
max_pss     | MB   | “Proportional Set Size”; amount of memory shared with other processes, accounted in a way that the amount is divided evenly between the processes that share it (Linux only)
io_in       | MB   | the number of MB read (cumulative)
io_out      | MB   | the number of MB written (cumulative)
mean_load   |      | CPU usage over time, divided by the total running time (first row)
cpu_time    | sec  | CPU time summed for user and system

<hr style="border:1px solid gray"> </hr>

### Run Times

---

```{r time_tool_length, echo=FALSE}
bench_df %>%
  ggplot(aes(x = runtime / 60, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Run time (min)",
    title = "Run times for classifying 10k fragments",
    color = "Length"
  )
```
```{r meantime_tool_length, echo=FALSE, message=FALSE}
bench_df %>%
  group_by(tool, length) %>%
  summarize(mean_runtime = mean(runtime)) %>%
  ggplot(aes(x = mean_runtime / 60, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Run time (min)",
       title = "Average run time for classifying 10k fragments",
       fill = "Length")
```

<hr style="border:1px solid gray"> </hr>

### Read and Write Operations

---

#### Read Operations

---

```{r read, echo=FALSE}
bench_df %>%
  ggplot(aes(x = io_in, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Read Operations (MB)",
    title = "Read operations for classifying 10k fragments",
    color = "Length"
  )
```

```{r mean_read, echo=FALSE, message=FALSE}
bench_df %>%
  group_by(tool, length) %>%
  summarize(mean_read = mean(io_in)) %>%
  ggplot(aes(x = mean_read, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Mean Read Operations (MB)",
       title = "Average read operations for classifying 10k fragments",
       fill = "Length")
```

#### Write Operations

---

```{r write, echo=FALSE}
bench_df %>%
  ggplot(aes(x = io_out, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Write Operations (MB)",
    title = "Write operations for classifying 10k fragments",
    color = "Length"
  )
```

```{r mean_write, echo=FALSE, message=FALSE}
bench_df %>%
  group_by(tool, length) %>%
  summarize(mean_write = mean(io_out)) %>%
  ggplot(aes(x = mean_write, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Mean Write Operations (MB)",
       title = "Average write operations for classifying 10k fragments",
       fill = "Length")
```

## General Classification Perfomance

Here we will look at the general classification performance of the classifiers. This general analysis only looks at "viral" vs. "non-viral" classes. Refined analysis will be shown later.

At this point, we can note that *Unlimited Breadsticks* did not identify any contigs as "viral". However, the tool seems to be running. This can be seen from the fact that in the resource usage analysis, Unlimited Breadsticks had longer compute time for longer contigs.

```{r metrics_function, echo = FALSE}
get_metrics <- function(df) {
  metrics <- tibble(
    tool = factor(),
    length = factor(),
    tp = numeric(),
    fp = numeric(),
    tn = numeric(),
    fn = numeric(),
    F1 = numeric(),
    sensitivity = numeric(),
    specificity = numeric(),
    precision = numeric(),
    recall = numeric()
  )

  for (tool_i in levels(df$tool)) {
    for (length_i in levels(df$length)) {
      subset_df <- df %>%
        filter(tool == tool_i) %>%
        filter(length == length_i)
      
      cm <- confusionMatrix(subset_df$predict_class,
                            subset_df$actual_class,
                            positive = "viral")
      
      # Get number of total negatives and positives
      num_neg <- cm$table[1] + cm$table[2]
      num_pos <- cm$table[3] + cm$table[4]
      
      # Convert to relative amounts
      cm$table[1] <- cm$table[1] / num_neg
      cm$table[2] <- cm$table[2] / num_neg
      cm$table[3] <- cm$table[3] / num_pos
      cm$table[4] <- cm$table[4] / num_pos
      
      metrics <- metrics %>%
        add_row(
          tool = tool_i,
          length = length_i,
          tp = cm$table[4],
          fp = cm$table[2],
          tn = cm$table[1],
          fn = cm$table[3],
          F1 = cm$byClass["F1"],
          specificity = cm$byClass["Specificity"],
          sensitivity = cm$byClass["Sensitivity"],
          precision = cm$byClass["Precision"],
          recall = cm$byClass["Recall"]
        )
    }
  }
  
  metrics
}
```


```{r, echo = FALSE}
factor_columns <- c("length","tool", "actual_class", "predict_class", "order")

class_df <- class_df %>% 
  mutate_each(funs(factor(.)), factor_columns)

metrics <- class_df %>% 
  get_metrics() %>% 
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))

metrics
```

<hr style="border:1px solid gray"> </hr>

### *F1* Score by Tool

---

Definition:

* $F1=\frac{2*precision*recall}{precision+recall}$

*F1*-score is the harmonic mean of precision and recall.

```{r, echo=FALSE}
metrics %>%
  ggplot(aes(x = F1, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(x = "F1 Score",
       y = "",
       fill = "Length")
```

Next, lets see how dependent *F1*-score is on length more explicitly.


```{r f_length, echo=FALSE}
metrics %>%
  mutate(length = as.numeric(as.character(length))) %>%
  ggplot(aes(x = length, y = F1, color = tool)) +
  geom_line(alpha = 0.5) +
  geom_point(size = 2) +
  geom_label_repel(
    data = metrics %>%
      mutate(length = as.numeric(as.character(length))) %>%
      filter(length == max(length)),
    aes(label = tool),
    xlim = 5500,
    label.size = NA,
    label.padding = 0
  ) +
  scale_color_hurwitz("distinguish") +
  theme(legend.position = "none") +
  xlim(c(NA, 6500)) +
  labs(x = "Fragment length (nt)",
       y = "F1-score",
       color = "Tool")
```

<hr style="border:1px solid gray"> </hr>

### Precision vs. Recall

---

Definitions:

* $precision = \frac{TP}{TP + FP}$
* $recall = sensitivity = \frac{TP}{TP + FN}$

These are the components that determine *F1*-score. Those with high *F1*-score will have points in the top right of their precision-recall plot.

```{r, echo = FALSE}
metrics %>%
  ggplot(aes(y = precision, x = recall, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Length")
```

The animation below shows how the precision and recall of each tool shifts based on length.

```{r, echo = FALSE, results = FALSE}
anim <- metrics %>%
  ggplot(aes(y = precision, x = recall, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Tool") +
  transition_states(length) +
  ggtitle("Precision vs. Recall",
          subtitle = "Length = {closest_state}")

anim_save(
  "figures/animated_precision_recall.gif",
  anim,
  width = 600,
  height = 500
)
```

![](figures/animated_precision_recall.gif)

<hr style="border:1px solid gray"> </hr>

### Sensitivity vs. Specificity

---

Definitions:

* $sensitivity = \frac{TP}{TP + FN}$
* $specificity = \frac{TN}{TN + FP}$

Often the tradeoff is sensitivity vs. specificity. For instance, a reference-based method may be expected to detect fewer viral contigs (low sensitivity), but also have few false positives (high specificity). The highest performing tools would have both high sensitivity and specificity, and would have point toward the top right of their sensitivity vs specificity plot.

**Some notable trends**:

Several tools have very high specificity:

* VIBRANT
* viralVerify
* VirSorter
* VirSorter2

VirSorter, however has very poor sensitivity.

Some tools have high sensitivity:

* DeepVirFinder
* MetaPhinder
* VirFinder
* Seeker

Seeker has the lowest specificity of all.

Those tools with high specificity (VIBRANT, viralVerify, VirSorter2) have sensitivity that is more dependent on contig length than those with generally higher sensitivity (DeepVirFinder, MetaPhinder, Seeker, VirFinder)

```{r, echo=FALSE}
metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "Sensitivity",
       y = "Specificity",
       color = "Length")
```

Here is a similar plot, but animated along length.

```{r, echo = FALSE, results = FALSE}
anim <- metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Specificity",
       x = "Sensitivity",
       color = "Tool") +
  transition_states(length) +
  ggtitle("Specificity vs. Sensitivity",
          subtitle = "Length = {closest_state}")

anim_save(
  "figures/animated_specificity_sensitivity.gif",
  anim,
  width = 600,
  height = 500
)
```

![](figures/animated_specificity_sensitivity.gif)

<hr style="border:1px solid gray"> </hr>

### ROC Snapshot

---

This is not a true Receiver Operating Characteristic curve since I am only plotting  discrete points. I may try to plot real ROC, but that will only be possible for probabilistic classifiers that output their probabilities.

The axes shown here are the components that constitute precision. Tools with high precision will have points in the top left corner of the TP vs FP plot.

```{r, echo=FALSe}
metrics %>%
  ggplot(aes(x = fp, y = tp, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Length")
```

```{r, echo = FALSE, results = FALSE}
anim <- metrics %>%
  ggplot(aes(x = fp, y = tp, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Tool") +
  transition_states(length) +
  ggtitle("TPR vs. FPR",
          subtitle = "Length = {closest_state}")

anim_save("figures/animated_tpr_fpr.gif",
          anim,
          width = 600,
          height = 500)
```

![](figures/animated_tpr_fpr.gif)

## Viral Taxonomy

### Top orders

---

Most highly represented viral orders.

I left the `NA`s for now, to get an idea of their magnitude

```{r prokarya, echo=FALSE}
viral <- class_df %>%
  filter(actual == "viral")

viral %>%
  group_by(order) %>%
  count() %>%
  mutate(num_frags = n / length(levels(viral$tool))) %>%
  select(-n) %>%
  arrange(desc(num_frags))
```
<hr style="border:1px solid gray"> </hr>

### Distribution

---

```{r order_histo, echo=FALSE}
viral %>%
  mutate(order = if_else(is.na(order), "unknown", as.character(order))) %>%
  group_by(order) %>%
  count() %>%
  mutate(num_frags = n / length(levels(viral$tool))) %>%
  select(-n) %>%
  mutate(is_na = (order == "unknown")) %>%
  ggplot(aes(
    x = fct_reorder(order,-num_frags),
    y = num_frags / 40000,
    color = is_na,
    fill = is_na
  )) +
  scale_color_hurwitz("contrast") +
  scale_fill_hurwitz("contrast") +
  geom_col() +
  theme(
    axis.text.x = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    legend.position = "none",
    plot.caption = element_text(hjust = 0.5)
  ) +
  labs(
    title = "Distribution of viral orders",
    subtitle = "Red: Unretrieved taxonomy",
    x = "Viral order",
    y = "Proportion of fragments",
    caption = "Caudovirales makes up >50% of fragments"
  )
```


<hr style="border:1px solid gray"> </hr>

### Performance Differences

---

Were the true positive (TP) and false negative (FN) rates different for caudovirales and other orders?

This is only looking at fragments that were truly viral, so *F1*-score, etc are not relevant.

```{r, echo = FALSE}
caudo_metrics <- viral %>% 
  filter(order == "Caudovirales") %>% 
  get_metrics() %>% 
  mutate(caudo = "Caudovirales")

noncaudo_metrics <- viral %>% 
  filter(order != "Caudovirales") %>% 
  get_metrics() %>% 
  mutate(caudo = "other")

viral_metrics <- bind_rows(caudo_metrics, noncaudo_metrics)

viral_metrics %>%
  ggplot(aes(x = as.integer(length)/1000, y= sensitivity, color = caudo)) +
  facet_wrap(~ tool) +
  geom_point() +
  labs(x = "Fragment length (1000 nt)",
       y = "Sensitivity",
       color = "")
```



## Binary Classification

This analysis only includes phage and bacteria.

```{r, echo=FALSE}
two_class <- class_df %>% 
  filter(actual %in% c('bacteria', 'viral'))

binary_metrics <- get_metrics(two_class) %>%
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))

binary_metrics
```


<hr style="border:1px solid gray"> </hr>

### *F1* Score by Tool

---

```{r, echo=FALSE}
binary_metrics %>%
  ggplot(aes(x = F1, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(x = "F1 Score",
       y = "",
       fill = "Length")
```

```{r, echo=FALSE}
binary_metrics %>%
  mutate(length = as.numeric(as.character(length))) %>%
  ggplot(aes(x = length, y = F1, color = tool)) +
  geom_line(alpha = 0.5) +
  geom_point(size = 2) +
  geom_label_repel(
    data = binary_metrics %>%
      mutate(length = as.numeric(as.character(length))) %>%
      filter(length == max(length)),
    aes(label = tool),
    xlim = 5500,
    ylim = c(0, 1),
    label.size = NA,
    label.padding = 0
  ) +
  scale_color_hurwitz("distinguish") +
  theme(legend.position = "none") +
  xlim(c(NA, 6500)) +
  ylim(c(NA, 1)) +
  labs(x = "Fragment length (nt)",
       y = "F1-score",
       color = "Tool")
```

<hr style="border:1px solid gray"> </hr>

### Precision vs. Recall

---


```{r, echo = FALSE}
binary_metrics %>%
  ggplot(aes(y = precision, x = recall, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Length")
```

```{r, echo = FALSE, results = FALSE}
anim <- binary_metrics %>%
  ggplot(aes(y = precision, x = recall, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Tool") +
  transition_states(length) +
  ggtitle("Precision vs. Recall",
          subtitle = "Length = {closest_state}")

anim_save(
  "figures/animated_binary_precision_recall.gif",
  anim,
  width = 600,
  height = 500
)
```

![](figures/animated_binary_precision_recall.gif)

<hr style="border:1px solid gray"> </hr>

### Sensitivity vs. Specificity

---

```{r, echo=FALSE}
binary_metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "Sensitivity",
       y = "Specificity",
       color = "Length")
```

```{r, echo = FALSE, results = FALSE}
anim <- binary_metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Specificity",
       x = "Sensitivity",
       color = "Tool") +
  transition_states(length) +
  ggtitle("Specificity vs. Sensitivity",
          subtitle = "Length = {closest_state}")

anim_save(
  "figures/animated_binary_specificity_sensitivity.gif",
  anim,
  width = 600,
  height = 500
)
```

![](figures/animated_binary_specificity_sensitivity.gif)

<hr style="border:1px solid gray"> </hr>

### ROC Snapshot

---

```{r, echo=FALSe}
binary_metrics %>%
  ggplot(aes(x = fp, y = tp, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Length")
```

```{r, echo = FALSE, results = FALSE}
anim <- binary_metrics %>%
  ggplot(aes(x = fp, y = tp, color = tool)) +
  geom_point(size = 3) +
  scale_color_brewer(palette = "Dark2") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Tool") +
  transition_states(length) +
  ggtitle("TPR vs. FPR",
          subtitle = "Length = {closest_state}")

anim_save("figures/animated_binary_tpr_fpr.gif",
          anim,
          width = 600,
          height = 500)
```

![](figures/animated_binary_tpr_fpr.gif)

