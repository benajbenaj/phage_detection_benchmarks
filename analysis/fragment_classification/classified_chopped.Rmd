---
title: "Chopped Genome Classifications"
output:
  html_document:
    df_print: paged
---

# {.tabset}

## Overview

### Experiment Description

---

#### Genome Chopping

---

Complete genomes were downloaded from RefSeq from the four "kingdoms": archaea, bacteria, fungi, and virus.

These genomes were "chopped" into non-overlapping fragments of the following lengths: 500, 1000, 3000, and 5000 nt.

The program `src/genome_chopper/chopper.py` was used to chop the genomes. Chopped genomes were written to `data/chopped` with 1 file per genome containing all the fragments of a given size for that genome.

The total number of complete genomes that were chopped from each category are as follows:

Kingdom  | No. Chopped Genomes
:-:      | :-:
archaea  | 758
bacteria | 43,576
fungi    | 36
virus    | 22,312

<hr style="border:1px solid gray"> </hr>

#### Fragment selection

---

The following quantity of genome fragments were selected:

Kingdom  | 500 nt | 1000 nt | 3000 nt | 5000 nt
:-:      | :-:    | :-:     | :-:     | :-:
archaea  | 10k    | 10k     | 10k     | 10k
bacteria | 10k    | 10k     | 10k     | 10k
fungi    | 10k    | 10k     | 10k     | 10k
viruses    | 10k    | 10k     | 10k     | 10k

Fragment selection was done using the Snakemake pipeline in `src/data_selection/`. Currently, it was performed with replacement. The selected fragments were written to `data/selected_frags/`. Other folders may be added, such as `data/selected_frags_1/`, `data/selected_frags_2/`, *etc.* for replicates.

<hr style="border:1px solid gray"> </hr>

#### Classification

---

The selected genome fragments were classified with the following tools:

* DeepVirFinder
* MetaPhinder
* Seeker
* Unlimited Breadsticks
* VIBRANT
* ViralVerify
* VirFinder
* VirSorter
* VirSorter2

Classification was run using the Snakemake pipeline in `src/classify_chopped/`.

Results were written to `data/classified_chopped/`.

All classifications are combined into the file `data/classified_chopped/combined_out/combined.csv`

This file was cleaned, completed, and had taxonomy added to it with `src/data_analysis/clean_classifications.R`.

<hr style="border:1px solid gray"> </hr>

#### Resource Usage

---

Snakemake benchmark command was used to record resource usage. Individual files are written to `src/classify_chopped/benchmarks/`. I may want to change that and have them written to the `data/classified_chopped_n/` folder(s).

All benchmark files are combined and written to `data/classified_chopped/combined_out/combined_benchmarks.csv`.

<hr style="border:1px solid gray"> </hr>

#### Taxonomy Information

---

Taxonomy information for each fragment was acquired by linking sequence ID -> assembly accession ID -> taxonomic ID -> taxonomy.

The scripts for doing so are in `src/data_analysis`

The workflow was:

`extract_ids.R` -> `get_tax_ids.R` -> `get_taxonomy.R`

The final file containing taxonomy of all genomes that were chopped is `data/refseq_info/taxonomy.csv`

<hr style="border:1px solid gray"> </hr>

### This Analysis

---

All analysis included here is conducted on the combined files:

* `data/classified_chopped/combined_out/cleaned_combined.csv`
* `data/classified_chopped/combined_out/cleaned_benchmarks.csv`

```{r imports, echo = FALSE}
library(caret)
library(dplyr)
library(forcats)
library(gganimate)
library(ggplot2)
library(ggrepel)     # Line plot text labels
library(gifski)      # Render animations
library(gt)
library(hurwitzLab)  # Colors and palettes
library(PRROC)
library(RColorBrewer)
library(stringr)     # str_glue()
library(svglite)
library(taxonomizr)
library(tibble)
library(tidyr)
library(viridis)
library(webshot)

# Set ggplot2 theme for whole R session
theme_set(theme_light() +
            theme(plot.title = element_text(hjust = 0.5),
                  plot.subtitle = element_text(hjust = 0.5)))

# Read in all classifications
class_df <-
  read.csv('../../data/classified_chopped/combined_out/cleaned_combined.csv') %>%
  mutate(across(
    c(length, tool, actual_class, predict_class, order),
    as.factor
  ))

# Read in Snakemake benchmarks
bench_df <-
  read.csv('../../data/classified_chopped/combined_out/cleaned_benchmarks.csv') %>% 
  mutate() %>% 
  mutate(across(
    c(length, tool, kingdom),
    as.factor
  ), runtime = runtime / 60,
  cup_time = cpu_time / 60) %>% 
  filter(!(tool == "VIBRANT" & length == 500),
         !(tool == "VirSorter" & length == 500))

bench_summary <- bench_df %>%
  group_by(tool, length) %>%
  summarize(
    n = n(),
    mean_runtime = mean(runtime),
    runtime_sd = sd(runtime),
    runtime_se = runtime_sd / sqrt(n),
    runtime_ci = runtime_se * qt((0.95) / 2 + 0.5, n - 1),
    mean_cputime = mean(cpu_time),
    cputime_sd = sd(cpu_time),
    cputime_se = cputime_sd / sqrt(n),
    cputime_ci = cputime_se * qt((0.95) / 2 + 0.5, n - 1),
    mean_read = mean(io_in),
    read_sd = sd(io_in),
    read_se = read_sd / sqrt(n),
    read_ci = read_se * qt((0.95) / 2 + 0.5, n - 1),
    mean_write = mean(io_out),
    write_sd = sd(io_out),
    write_se = write_sd / sqrt(n),
    write_ci = write_se * qt((0.95) / 2 + 0.5, n - 1)
  )

# bench_df[bench_df$tool == "VIBRANT" && bench_df$length == 500] = NA
# bench_df[bench_df$tool == "VirSorter" && bench_df$length == 500] = NA
```


## Resource Usage

### Recorded Parameters

---

Snakemake benchmark files have the following columns:

Column Name | Unit | Description
:-:         | :-:  | :--
s           | sec  | Running time in seconds
h:m:s       |      | Running time in hour:minute:sec
max_rss     | MB   | Maximum "Resident Set Size”; non-swapped physical memory used
max_vsm     | MB   | Maximum “Virtual Memory Size”; total amount of virtual memory used
max:uss     | MB   | “Unique Set Size”; memory which is unique to a process and which would be freed if the process was terminated right now
max_pss     | MB   | “Proportional Set Size”; amount of memory shared with other processes, accounted in a way that the amount is divided evenly between the processes that share it (Linux only)
io_in       | MB   | the number of MB read (cumulative)
io_out      | MB   | the number of MB written (cumulative)
mean_load   |      | CPU usage over time, divided by the total running time (first row)
cpu_time    | sec  | CPU time summed for user and system

<hr style="border:1px solid gray"> </hr>

### Run Times

---

```{r time_tool_length, echo=FALSE, message=FALSE}
runtime_tool_length_dotplot <- bench_df %>%
  ggplot(aes(x = runtime, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Run time (min)",
    color = "Length"
  )

ggsave(
  "figures/runtime_tool_length_dotplot.svg",
  runtime_tool_length_dotplot,
  width = 6,
  height = 4
)

ggsave(
  "figures/runtime_tool_length_dotplot.png",
  runtime_tool_length_dotplot,
  width = 6,
  height = 4
)

runtime_tool_length_dotplot

rm(runtime_tool_length_dotplot)
```

```{r meantime_tool_length, echo=FALSE, message=FALSE}
avgruntime_tool_length_col <- bench_summary %>%
  ggplot(aes(x = mean_runtime, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  geom_errorbar(
    aes(
      xmin = mean_runtime - runtime_ci,
      xmax = mean_runtime + runtime_ci,
      color = length,
      y = tool
    ),
    position = "dodge",
    width = 0.75,
    show.legend = FALSE
  ) +
  scale_fill_hurwitz("distinguish") +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Run time (min)",
       fill = "Length")

avgruntime_tool_length_col

ggsave(
  "figures/avgruntime_tool_length_col.svg",
  avgruntime_tool_length_col,
  width = 6,
  height = 4
)

ggsave(
  "figures/avgruntime_tool_length_col.png",
  avgruntime_tool_length_col,
  width = 6,
  height = 4
)



rm(avgruntime_tool_length_col)
```

```{r, echo=FALSE, message=FALSE}
runtime_table <- bench_summary %>%
  group_by(tool, length) %>%
  select(tool, length, mean_runtime, runtime_sd) %>% 
  rename("mean" = mean_runtime, "sd" = runtime_sd) %>%
  mutate(mean = signif(mean, 3), sd = signif(sd, 3)) %>% 
  ungroup() %>%
  pivot_wider(names_from = length, values_from = c(mean, sd)) %>%
  mutate(across(everything(), as.character)) %>%
  gt() %>%
  cols_merge(c(mean_500, sd_500), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_1000, sd_1000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_3000, sd_3000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_5000, sd_5000), pattern = "{1} ({2})") %>%
  cols_label(mean_500 = "500") %>%
  cols_label(mean_1000 = "1000") %>%
  cols_label(mean_3000 = "3000") %>%
  cols_label(mean_5000 = "5000")

runtime_table

gtsave(runtime_table, "tables/runtime_table.rtf")

rm(runtime_table)

```


<hr style="border:1px solid gray"> </hr>

### CPU Times

---

```{r, echo=FALSE, message=FALSE}
cputime_tool_length_dotplot <- bench_df %>%
  ggplot(aes(x = cpu_time, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "CPU time (min)",
    color = "Length"
  )

cputime_tool_length_dotplot

ggsave(
  "figures/cputime_tool_length_dotplot.svg",
  cputime_tool_length_dotplot,
  width = 6,
  height = 4
)

ggsave(
  "figures/cputime_tool_length_dotplot.png",
  cputime_tool_length_dotplot,
  width = 6,
  height = 4
)

rm(cputime_tool_length_dotplot)
```

```{r, echo=FALSE, message=FALSE}
avgcputime_tool_length_col <- bench_summary %>%
  ggplot(aes(x = mean_cputime, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  geom_errorbar(
    aes(
      xmin = mean_cputime - cputime_ci,
      xmax = mean_cputime + cputime_ci,
      color = length,
      y = tool
    ),
    position = "dodge",
    width = 0.75,
    show.legend = FALSE
  ) +
  scale_fill_hurwitz("distinguish") +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "CPU time (min)",
       fill = "Length")

avgcputime_tool_length_col

ggsave(
  "figures/avgcputime_tool_length_col.svg",
  avgcputime_tool_length_col,
  width = 6,
  height = 4
)

ggsave(
  "figures/avgcputime_tool_length_col.png",
  avgcputime_tool_length_col,
  width = 6,
  height = 4
)

rm(avgcputime_tool_length_col)
```

```{r, echo=FALSE, message=FALSE}
cputime_table <- bench_summary %>%
  group_by(tool, length) %>%
  select(tool, length, mean_cputime, cputime_sd) %>% 
  rename("mean" = mean_cputime, "sd" = cputime_sd) %>%
  mutate(mean = signif(mean, 3), sd = signif(sd, 3)) %>% 
  ungroup() %>%
  pivot_wider(names_from = length, values_from = c(mean, sd)) %>%
  mutate(across(everything(), as.character)) %>%
  gt() %>%
  cols_merge(c(mean_500, sd_500), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_1000, sd_1000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_3000, sd_3000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_5000, sd_5000), pattern = "{1} ({2})") %>%
  cols_label(mean_500 = "500") %>%
  cols_label(mean_1000 = "1000") %>%
  cols_label(mean_3000 = "3000") %>%
  cols_label(mean_5000 = "5000")

cputime_table

gtsave(cputime_table, "tables/cputime_table.rtf")

rm(cputime_table)

```


<hr style="border:1px solid gray"> </hr>

### Read and Write Operations

---

```{r}
readwrite_tool_length_dotplot <- bench_df %>%
  rename("Read" = io_in, "Write" = io_out) %>%
  pivot_longer(c(Read, Write), names_to = "operation", values_to = "mem") %>%
  ggplot(aes(x = mem, y = tool, color = length)) +
  facet_wrap(~operation, scales = "free_x") +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "IO (MB)",
    color = "Length"
  )

readwrite_tool_length_dotplot

ggsave(
  "figures/readwrite_tool_length_dotplot.svg",
  readwrite_tool_length_dotplot,
  width = 8,
  height = 4
)

ggsave(
  "figures/readwrite_tool_length_dotplot.png",
  readwrite_tool_length_dotplot,
  width = 8,
  height = 4
)

rm(readwrite_tool_length_dotplot)

```

```{r, echo=FALSE, message=FALSE}
io_summary <- bench_summary %>%
  group_by(tool, length) %>%
  select(tool, length, mean_read, read_sd, mean_write, write_sd) %>%
  mutate(
    mean_read = signif(mean_read, 3),
    read_sd = signif(read_sd, 3),
    mean_write = signif(mean_write, 3),
    write_sd = signif(write_sd, 3)
  ) %>%
  ungroup() %>%
  pivot_wider(
    names_from = length,
    values_from = c(mean_read, read_sd, mean_write, write_sd)
  ) %>% 
  mutate(across(everything(), as.character))

io_table_cols = head(io_summary, 1) %>% # Take the first row of a column
  mutate_all(as.character) %>% # prevents errors in pivot_longer
  pivot_longer(everything(), names_to = "column", values_to = "value") %>%
  select(-value) %>%
  mutate(label = str_replace(column, ".*_(\\d+)", "\\1")) %>%
  deframe()

io_table = io_summary %>% 
  gt() %>%
  cols_merge(c(mean_read_500, read_sd_500), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_read_1000, read_sd_1000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_read_3000, read_sd_3000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_read_5000, read_sd_5000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_write_500, write_sd_500), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_write_1000, write_sd_1000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_write_3000, write_sd_3000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_write_5000, write_sd_5000), pattern = "{1} ({2})") %>%
  tab_spanner(label = "Read", columns = matches("read")) %>% 
  tab_spanner(label = "Write", columns = matches("write")) %>% 
  cols_label(.list = io_table_cols)

io_table
gtsave(io_table, "tables/io_table.rtf")
rm(io_table, io_table_cols, io_summary)
```

#### Read Operations

---

```{r read, echo=FALSE}
readoperations_tool_length_dotplot <- bench_df %>%
  ggplot(aes(x = io_in, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Read Operations (MB)",
    color = "Length"
  )

ggsave(
  "figures/readoperations_tool_length_dotplot.svg",
  readoperations_tool_length_dotplot,
  width = 6,
  height = 4
)

ggsave(
  "figures/readoperations_tool_length_dotplot.png",
  readoperations_tool_length_dotplot,
  width = 6,
  height = 4
)

readoperations_tool_length_dotplot
rm(readoperations_tool_length_dotplot)

```

```{r mean_read, echo=FALSE, message=FALSE}
avgreadoperations_tool_length_col <- bench_summary %>%
  ggplot(aes(x = mean_read, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  geom_errorbar(
    aes(
      xmin = mean_read - read_sd,
      xmax = mean_read + read_sd,
      color = length,
      y = tool
    ),
    position = "dodge",
    width = 0.75,
    show.legend = FALSE
  ) +
  scale_fill_hurwitz("distinguish") +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Mean Read Operations (MB)",
       fill = "Length")

avgreadoperations_tool_length_col

ggsave(
  "figures/avgreadoperations_tool_length_col.svg",
  avgreadoperations_tool_length_col,
  width = 6,
  height = 4
)

ggsave(
  "figures/avgreadoperations_tool_length_col.png",
  avgreadoperations_tool_length_col,
  width = 6,
  height = 4
)


rm(avgreadoperations_tool_length_col)
```

```{r, echo=FALSE, message=FALSE}
read_table <- bench_summary %>%
  group_by(tool, length) %>%
  select(tool, length, mean_read, read_sd) %>% 
  rename("mean" = mean_read, "sd" = read_sd) %>%
  mutate(mean = signif(mean, 3), sd = signif(sd, 3)) %>%
  ungroup() %>%
  pivot_wider(names_from = length, values_from = c(mean, sd)) %>%
  mutate(across(everything(), as.character)) %>%
  gt() %>%
  cols_merge(c(mean_500, sd_500), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_1000, sd_1000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_3000, sd_3000), pattern = "{1} ({2})") %>%
  cols_merge(c(mean_5000, sd_5000), pattern = "{1} ({2})") %>%
  cols_label(mean_500 = "500") %>%
  cols_label(mean_1000 = "1000") %>%
  cols_label(mean_3000 = "3000") %>%
  cols_label(mean_5000 = "5000")

read_table
gtsave(read_table, "tables/read_table.rtf")
rm(read_table)
```

#### Write Operations

---

```{r write, echo=FALSE}
writeoperations_tool_length_dotplot <- bench_df %>%
  ggplot(aes(x = io_out, y = tool, color = length)) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(
    y = "",
    x = "Write Operations (MB)",
    color = "Length"
  )

ggsave(
  "figures/writeoperations_tool_length_dotplot.svg",
  writeoperations_tool_length_dotplot,
  width = 6,
  height = 4
)

ggsave(
  "figures/writeoperations_tool_length_dotplot.png",
  writeoperations_tool_length_dotplot,
  width = 6,
  height = 4
)


writeoperations_tool_length_dotplot
rm(writeoperations_tool_length_dotplot)
```

```{r mean_write, echo=FALSE, message=FALSE}
avgwriteoperations_tool_length_col <- bench_df %>%
  group_by(tool, length) %>%
  summarize(mean_write = mean(io_out)) %>%
  ggplot(aes(x = mean_write, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(y = "",
       x = "Mean Write Operations (MB)",
       fill = "Length")

ggsave(
  "figures/avgwriteoperations_tool_length_col.svg",
  avgwriteoperations_tool_length_col,
  width = 6,
  height = 4
)

ggsave(
  "figures/avgwriteoperations_tool_length_col.png",
  avgwriteoperations_tool_length_col,
  width = 6,
  height = 4
)

avgwriteoperations_tool_length_col
rm(avgwriteoperations_tool_length_col)
```
#### Combined figure

```{r, echo=FALSE}
readwritecpu_tool_length_dotplot <- bench_df %>%
  mutate(cpu_time = cpu_time / 60) %>%
  rename(
    "A. Read (MB)" = io_in,
    "B. Write (MB)" = io_out,
    "C. CPU Time (hr)" = cpu_time
  ) %>%
  pivot_longer(c("A. Read (MB)", "B. Write (MB)", "C. CPU Time (hr)"),
               names_to = "operation",
               values_to = "val") %>%
  ggplot(aes(x = val, y = tool, color = length)) +
  facet_wrap( ~ operation,
              scales = "free_x",
              nrow = 2) +
  geom_jitter(alpha = 0.5, height = 0.3) +
  scale_color_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  theme(legend.position = "bottom") +
  labs(y = "",
       x = "",
       color = "Length")

readwritecpu_tool_length_dotplot

ggsave(
  "figures/readwritecpu_tool_length_dotplot.svg",
  readwritecpu_tool_length_dotplot,
  width = 8,
  height = 6, standalone=FALSE
)

ggsave(
  "figures/readwritecpu_tool_length_dotplot.png",
  readwritecpu_tool_length_dotplot,
  width = 8,
  height = 4
)

rm(readwritecpu_tool_length_dotplot)

```

## General Classification Perfomance

Here we will look at the general classification performance of the classifiers. This general analysis only looks at "viral" vs. "non-viral" classes. Refined analysis will be shown later.

At this point, we can note that *Unlimited Breadsticks* did not identify any contigs as "viral". However, the tool seems to be running. This can be seen from the fact that in the resource usage analysis, Unlimited Breadsticks had longer compute time for longer contigs.

```{r metrics_function, echo = FALSE}
get_metrics <- function(df) {
  metrics <- tibble(
    tool = factor(),
    length = factor(),
    tp = numeric(),
    fp = numeric(),
    tn = numeric(),
    fn = numeric(),
    F1 = numeric(),
    sensitivity = numeric(),
    specificity = numeric(),
    precision = numeric(),
    recall = numeric(),
    original_precision = numeric()
  )

  for (tool_i in levels(df$tool)) {
    for (length_i in levels(df$length)) {
      subset_df <- df %>%
        filter(tool == tool_i) %>%
        filter(length == length_i)
      
      cm <- confusionMatrix(subset_df$predict_class,
                            subset_df$actual_class,
                            positive = "viral")
      
      # Get number of total negatives and positives
      num_neg <- cm$table[1] + cm$table[2]
      num_pos <- cm$table[3] + cm$table[4]
      
      # Convert to relative amounts
      cm$table[1] <- cm$table[1] / num_neg
      cm$table[2] <- cm$table[2] / num_neg
      cm$table[3] <- cm$table[3] / num_pos
      cm$table[4] <- cm$table[4] / num_pos
      
      metrics <- metrics %>%
        add_row(
          tool = tool_i,
          length = length_i,
          tp = cm$table[4],
          fp = cm$table[2],
          tn = cm$table[1],
          fn = cm$table[3],
          original_precision = cm$byClass["Precision"],
        )
    }
  }
  metrics <- metrics %>% 
    mutate(precision = tp / (tp + fp),
           sensitivity = tp / (tp + fn),
           specificity = tn / (tn + fp),
           recall = sensitivity,
           F1 = 2 * precision * sensitivity / (precision + sensitivity))
  metrics
}
```


```{r, echo = FALSE}
metrics <- class_df %>% 
  get_metrics() %>% 
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))

metrics
```


<hr style="border:1px solid gray"> </hr>

### *F1* Score by Tool

---

Definition:

* $F1=\frac{2*precision*recall}{precision+recall}$

*F1*-score is the harmonic mean of precision and recall.

```{r, echo=FALSE}
metrics %>%
  ggplot(aes(x = F1, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_hurwitz("distinguish") +
  scale_y_discrete(limits = rev) +
  labs(x = "F1 Score",
       y = "",
       fill = "Length")
```

Next, lets see how dependent *F1*-score is on length more explicitly.


```{r f_length, echo=FALSE}
metrics %>%
  mutate(length = as.numeric(as.character(length))) %>%
  ggplot(aes(x = length, y = F1, color = tool)) +
  geom_line(alpha = 0.5) +
  geom_point(size = 2) +
  geom_label_repel(
    data = metrics %>%
      mutate(length = as.numeric(as.character(length))) %>%
      filter(length == max(length)),
    aes(label = tool),
    xlim = 5500,
    label.size = NA,
    label.padding = 0
  ) +
  scale_color_hurwitz("distinguish") +
  theme(legend.position = "none") +
  xlim(c(NA, 6500)) +
  labs(x = "Fragment length (nt)",
       y = "F1-score",
       color = "Tool")
```

<hr style="border:1px solid gray"> </hr>

### Precision vs. Recall

---

Definitions:

* $precision = \frac{TP}{TP + FP}$
* $recall = sensitivity = \frac{TP}{TP + FN}$

These are the components that determine *F1*-score. Those with high *F1*-score will have points in the top right of their precision-recall plot.

```{r, echo = FALSE}
metrics %>%
  ggplot(aes(y = precision, x = recall, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Length")
```

The animation below shows how the precision and recall of each tool shifts based on length.

```{r, echo = FALSE, results = FALSE}
# anim <- metrics %>%
#   ggplot(aes(y = precision, x = recall, color = tool)) +
#   geom_point(size = 3) +
#   scale_color_brewer(palette = "Dark2") +
#   lims(x = c(0, 1), y = c(0, 1)) +
#   labs(y = "Precision",
#        x = "Recall (Sensitivity)",
#        color = "Tool") +
#   transition_states(length) +
#   ggtitle("Precision vs. Recall",
#           subtitle = "Length = {closest_state}")
# 
# anim_save(
#   "figures/animated_precision_recall.gif",
#   anim,
#   width = 600,
#   height = 500
# )
```

![](figures/animated_precision_recall.gif)

<hr style="border:1px solid gray"> </hr>

### Sensitivity vs. Specificity

---

Definitions:

* $sensitivity = \frac{TP}{TP + FN}$
* $specificity = \frac{TN}{TN + FP}$

Often the tradeoff is sensitivity vs. specificity. For instance, a reference-based method may be expected to detect fewer viral contigs (low sensitivity), but also have few false positives (high specificity). The highest performing tools would have both high sensitivity and specificity, and would have point toward the top right of their sensitivity vs specificity plot.

**Some notable trends**:

Several tools have very high specificity:

* VIBRANT
* viralVerify
* VirSorter
* VirSorter2

VirSorter, however has very poor sensitivity.

Some tools have high sensitivity:

* DeepVirFinder
* MetaPhinder
* VirFinder
* Seeker

Seeker has the lowest specificity of all.

Those tools with high specificity (VIBRANT, viralVerify, VirSorter2) have sensitivity that is more dependent on contig length than those with generally higher sensitivity (DeepVirFinder, MetaPhinder, Seeker, VirFinder)

```{r, echo=FALSE}
metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "Sensitivity",
       y = "Specificity",
       color = "Length")
```

Here is a similar plot, but animated along length.

```{r, echo = FALSE, results = FALSE}
# anim <- metrics %>%
#   ggplot(aes(x = sensitivity, y = specificity, color = tool)) +
#   geom_point(size = 3) +
#   scale_color_brewer(palette = "Dark2") +
#   lims(x = c(0, 1), y = c(0, 1)) +
#   labs(y = "Specificity",
#        x = "Sensitivity",
#        color = "Tool") +
#   transition_states(length) +
#   ggtitle("Specificity vs. Sensitivity",
#           subtitle = "Length = {closest_state}")
# 
# anim_save(
#   "figures/animated_specificity_sensitivity.gif",
#   anim,
#   width = 600,
#   height = 500
# )
```

![](figures/animated_specificity_sensitivity.gif)

<hr style="border:1px solid gray"> </hr>

### ROC Snapshot

---

This is not a true Receiver Operating Characteristic curve since I am only plotting  discrete points. I may try to plot real ROC, but that will only be possible for probabilistic classifiers that output their probabilities.

The axes shown here are the components that constitute precision. Tools with high precision will have points in the top left corner of the TP vs FP plot.

```{r, echo=FALSE}
metrics %>%
  ggplot(aes(x = fp, y = tp, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Length")
```

```{r, echo = FALSE, results = FALSE}
# anim <- metrics %>%
#   ggplot(aes(x = fp, y = tp, color = tool)) +
#   geom_point(size = 3) +
#   scale_color_brewer(palette = "Dark2") +
#   lims(x = c(0, 1), y = c(0, 1)) +
#   labs(x = "False Positive Rate",
#        y = "True Positive Rate",
#        color = "Tool") +
#   transition_states(length) +
#   ggtitle("TPR vs. FPR",
#           subtitle = "Length = {closest_state}")
# 
# anim_save("figures/animated_tpr_fpr.gif",
#           anim,
#           width = 600,
#           height = 500)
```

![](figures/animated_tpr_fpr.gif)

## Viral Taxonomy

### Top orders

---

Most highly represented viral orders.

I left the `NA`s for now, to get an idea of their magnitude

```{r prokarya, echo=FALSE}
viral <- class_df %>%
  filter(actual == "viral")

viral %>%
  group_by(order) %>%
  count() %>%
  mutate(num_frags = n / length(levels(viral$tool))) %>%
  select(-n) %>%
  arrange(desc(num_frags))
```
<hr style="border:1px solid gray"> </hr>

### Distribution

---

```{r distribution_function, echo = FALSE}
plot_order_dist <- function(df) {
  df %>%
    mutate(order = if_else(is.na(order), "Unknown", as.character(order))) %>%
    group_by(order, length) %>%
    count() %>%
    mutate(num_frags = n / length(levels(viral$tool))) %>%
    mutate(
      order_grouping = case_when(
        order == "Caudovirales" ~ "Caudovirales",
        order == "Unknown" ~ "Unknown",
        TRUE ~ "All others"
      )
    ) %>%
    ggplot(aes(
      y = fct_reorder(order, num_frags),
      x = num_frags,
      color = fct_reorder(order_grouping, -num_frags),
      fill = fct_reorder(order_grouping, -num_frags)
    )) +
    scale_color_hurwitz("main") +
    scale_fill_hurwitz("main") +
    geom_col() +
    facet_wrap(~ length) +
    theme(
      axis.text.y = element_blank(),
      panel.grid.major.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.caption = element_text(hjust = 0.5)
    ) +
    labs(
      y = "Viral order",
      x = "Number of fragments",
      color = "Order",
      fill = "Order",
      title = "Fragment distribution by length and viral order"
    )
}

```


```{r order_histo, echo=FALSE}
viral %>%
  plot_order_dist() +
  labs(subtitle = "All viral fragments included")
  
```


<hr style="border:1px solid gray"> </hr>

### Performance Differences

---

Was sensitivity different for caudovirales and other orders?

This is only looking at fragments that were truly viral, so *F1*-score, etc are not relevant. All "viral" sequences are included here.

```{r, echo = FALSE}
metrics_by_phage_order <- function(df, other_label) {
  caudo_metrics <- df %>%
    filter(order == "Caudovirales") %>%
    get_metrics() %>%
    mutate(caudo = "Caudovirales")
  
  noncaudo_metrics <- df %>%
    filter(order != "Caudovirales") %>%
    get_metrics() %>%
    mutate(caudo = other_label)
  
  metrics <- bind_rows(caudo_metrics, noncaudo_metrics)
  
  metrics
}

plot_order_performance <- function(df) {
  
  
  df %>%
    ggplot(aes(
      x = as.integer(length) / 1000,
      y = sensitivity,
      color = fct_rev(caudo)
    )) +
    facet_wrap(~ tool) +
    geom_point() +
    scale_color_hurwitz("distinguish") +
    labs(
      x = "Fragment length (1000 nt)",
      y = "Sensitivity",
      color = "Order"
    )
}
```

```{r, echo = FALSE}
viral %>% 
  metrics_by_phage_order("Other viruses") %>% 
  plot_order_performance() +
  labs(subtitle = "All viral fragments included")
```


## Phages only

### Phage selection

---

Previous results include all viral sequences, of which many are eukaryotic viruses. To filter only for bacteriophage, I am using a string match on the species, searching for "phage" (case insensitive).

The distribution is even more skewed when only including phages. I would like to verify that my method of finding phages is sound.

```{r, echo = FALSE}
phages <- viral %>% 
  filter(str_detect(species, regex("phage", ignore_case = T)))

phages %>%
  plot_order_dist() +
  labs(subtitle = "Only phage fragments included") +
  theme(axis.text.y = element_text())
```

<hr style="border:1px solid gray"> </hr>

### Performance Differences

---

```{r, echo = FALSE}
caudo_v_non <- phages %>% 
  metrics_by_phage_order("Other phages")

caudo_v_non %>% 
  plot_order_performance()

ggsave(
  "figures/length_sensitivity_viral_order.svg",
  width = 4,
  height = 3, standalone=FALSE
)

ggsave(
  "figures/length_sensitivity_viral_orde.png",
  width = 4,
  height = 3
)
```


## Binary Classification

This analysis only includes phage (as determined previously) and bacteria. Because I am only including phage and not all viral fragments, the classes are imbalanced.

```{r, echo=FALSE}
bacteria <- class_df %>%
  filter(actual == "bacteria")

two_class <- bind_rows(bacteria, phages) %>%
  mutate(actual = if_else(actual == "viral", "phage", actual))

two_class %>%
  group_by(actual, length) %>%
  count() %>%
  mutate(num_frags = n / length(levels(two_class$tool))) %>%
  select(-n) %>%
  pivot_wider(names_from = length, values_from = num_frags) %>%
  rename("class" = "actual")

```
```{r, echo = FALSE}
binary_metrics <- get_metrics(two_class) %>%
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))

binary_metrics
```


<hr style="border:1px solid gray"> </hr>

### *F1* Score by Tool

---

```{r, echo=FALSE}
binary_metrics %>%
  ggplot(aes(x = F1, y = tool, fill = length)) +
  geom_col(position = "dodge",
           alpha = 0.75,
           width = 0.75) +
  scale_fill_brewer("Dark2") +
  scale_y_discrete(limits = rev) +
  labs(x = "F1 Score",
       y = "",
       fill = "Length")
```

```{r, echo=FALSE}
binary_metrics %>%
  mutate(length = as.numeric(as.character(length))) %>%
  ggplot(aes(x = length, y = F1, color = tool)) +
  geom_line(alpha = 0.5) +
  geom_point(size = 2) +
  # geom_label_repel(
  #   data = binary_metrics %>%
  #     mutate(length = as.numeric(as.character(length))) %>%
  #     filter(!is.na(precision)) %>% 
  #     group_by(tool) %>% 
  #     mutate(min_length = min(length)) %>% 
  #     filter(length == min_length),
  #   aes(label = tool),
  #   xlim = -50,
  #   ylim = c(0, 1),
  #   label.size = NA,
  #   label.padding = 0
  # ) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "none") +
  xlim(c(-50, NA)) +
  ylim(c(NA, 1)) +
  labs(x = "Fragment length (nt)",
       y = "F1 score",
       color = "Tool")

ggsave(
  "figures/length_f1_tool.svg",
  width = 8,
  height = 8, standalone=FALSE
)

ggsave(
  "figures/length_f1_tool.png",
  width = 8,
  height = 8
)
```

<hr style="border:1px solid gray"> </hr>

### Precision vs. Recall

---


```{r, echo = FALSE}
binary_metrics %>%
  ggplot(aes(y = precision, x = recall, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(y = "Precision",
       x = "Recall (Sensitivity)",
       color = "Length")

ggsave(
  "figures/recall_precision_length_tool.svg",
  width = 8,
  height = 8, standalone=FALSE
)

ggsave(
  "figures/recall_precision_length_tool.png",
  width = 8,
  height = 4
)
```


<hr style="border:1px solid gray"> </hr>

### Sensitivity vs. Specificity

---

```{r, echo=FALSE}
binary_metrics %>%
  ggplot(aes(x = sensitivity, y = specificity, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "Sensitivity",
       y = "Specificity",
       color = "Length")
```

<hr style="border:1px solid gray"> </hr>

### FPR vs TPR

---

```{r, echo=FALSE}
binary_metrics %>%
  ggplot(aes(x = fp, y = tp, color = length)) +
  facet_wrap( ~ tool) +
  geom_point(size = 2) +
  scale_color_hurwitz("distinguish") +
  lims(x = c(0, 1), y = c(0, 1)) +
  labs(x = "False Positive Rate",
       y = "True Positive Rate",
       color = "Length")
```

### Precision-Recall Curves

---

```{r, echo = FALSE}
calc_precision_recall <- function(df) {
  out_df <- tibble(
    tool = character(),
    length = character(),
    recall = numeric(),
    precision = numeric(),
    prob = numeric(),
    auc = numeric()
  )
  
  for (tool_i in unique(df$tool)) {
    tool_df <- df %>%
      filter(tool == tool_i)
    
    for (length_i in unique(tool_df$length)) {
      length_df <- tool_df %>%
        filter(length == length_i)
      
      viral_probs <-
        length_df$prob[length_df$actual_class == "viral"]
      nonviral_probs <-
        length_df$prob[length_df$actual_class == "non-viral"]
      
      pr <-
        pr.curve(
          scores.class0 = viral_probs,
          scores.class1 = nonviral_probs,
          curve = T
        )
      
      pr_curve <- as_tibble(pr$curve) %>%
        rename("recall" = V1,
               "precision" = V2,
               "prob" = V3)
      pr_curve$tool = tool_i
      pr_curve$length = length_i
      pr_curve$auc = pr$auc.integral
      
      out_df <- out_df %>%
        rbind(pr_curve)
    }
  }
  
  out_df
}
```


```{r, echo = FALSE}
auprc <- two_class %>%
  filter(!is.na(value), value != "-", tool != "VirSorter", tool != "VirSorter2") %>%
  group_by(tool) %>%
  mutate(
    value = as.numeric(value),
    min_val = min(value),
    max_val = max(value),
    prob = (value - min_val) / (max_val - min_val)
  ) %>%
  calc_precision_recall() %>%
  mutate(length = factor(length, levels = c("500", "1000", "3000", "5000")))
```

```{r, echo = FALSE}
precision_recall_curve <- auprc %>% 
  ggplot(aes(x = recall, y = precision, color = length)) +
  facet_wrap(~ tool) +
  geom_line() +
  scale_color_hurwitz("distinguish") +
  labs(x = "Recall",
       y = "Precision",
       color = "Length (nt)")

precision_recall_curve

ggsave(
  "figures/precision_recall_curve.svg",
  width = 6,
  height = 3.3, standalone=FALSE
)

ggsave(
  "figures/precision_recall_curve.png",
  width = 6,
  height = 3.3
)
```
```{r, echo = FALSE}
auprc %>%
  select(tool, length, auc) %>%
  distinct() %>%
  mutate(length = as.numeric(as.character(length)) / 1000) %>%
  ggplot(aes(x = length, y = auc)) +
  facet_wrap( ~ tool) +
  geom_line(linetype = "dotdash",
            alpha = 0.7,
            color = "darkgrey") +
  geom_point() +
  labs(x = "Fragment Length (1000 nt)",
       y = "AUPRC")

ggsave(
  "figures/auprc.svg",
  width = 6,
  height = 4, standalone=FALSE
)

ggsave(
  "figures/auprc.png",
  width = 6,
  height = 4
)
```


## Caudovirales

Here, I will dig deeper into the caudoviral sequences taxonomies.Again, these are only including seqeunces determined to be phage (not just viral)

### Top families

```{r, echo = FALSE}
caudo <- phages %>% 
  filter(order == "Caudovirales") %>% 
  group_by(family)

caudo %>%
  count() %>%
  mutate(num_frags = n / length(levels(viral$tool))) %>%
  select(-n) %>%
  arrange(desc(num_frags))
```

<hr style="border:1px solid gray"> </hr>

### Sensitivity by family

---

This plot becomes a bit complicated because there are lots of variables, and I cannot aggregate any of them, so here is how they are mapped:


* length - $x$
* sensitivity - $y$ 
* caudovirus family - $facet\_wrap$
* tool - $color$

To make it a little easier on the eyes, I also scaled the $alpha$ by log of number of fragments in each family.

```{r, echo = FALSE}
caudo_metrics <- caudo %>% 
  get_metrics() %>% 
  mutate(family = "combined")

for(family_i in levels(as.factor(caudo$family))) {
  family_df <- caudo %>%
    filter(family == family_i)
  
  family_metrics <- family_df %>% 
    get_metrics() %>% 
    mutate(family = family_i)
  
  caudo_metrics <- bind_rows(caudo_metrics, family_metrics)
}
```

<hr style="border:1px solid gray"> </hr>

### Top families

---


Alternatively, we can just look at the top (3) families

```{r}
top_caudo <- caudo %>%
  count() %>%
  mutate(num_frags = n / length(levels(viral$tool))) %>%
  ungroup() %>%
  top_n(n = 3)

top_caudo_metrics <- caudo_metrics %>%
  filter(family %in% top_caudo$family) %>%
  left_join(caudo %>%
              count() %>%
              mutate(num_frags = n / length(levels(viral$tool))) %>%
              select(-n))

top_caudo_metrics %>%
  ggplot(aes(
    x = as.integer(length) / 1000,
    y = sensitivity,
    color = family,
    alpha = num_frags
  )) +
  facet_wrap(~ tool) +
  geom_point(aes()) +
  scale_color_brewer(palette="Dark2") +
  scale_alpha(range = c(0.5, 1),
              trans = "log",
              guide = "none") +
  labs(
    x = "Fragment length (1000 nt)",
    y = "Sensitivity",
    color = "Family"
  )

ggsave(
  "figures/length_sensitivity_caudo_family.svg",
  width = 8,
  height = 6, standalone=FALSE
)

ggsave(
  "figures/length_sensitivity_caudo_family.png",
  width = 8,
  height = 6
)
```

## Eukaryotes

Plot fungal fragments representing eukaryotes

```{r, echo = FALSE}
fungi <-class_df %>% 
  filter(superkingdom == "Eukaryota")

fungi_metrics <- fungi %>% 
  get_metrics() %>% 
  mutate(length = as.numeric(length))
```

```{r, echo = FALSE}
fungi_metrics %>% 
  ggplot(aes(x = length / 1000, y = specificity)) +
  facet_wrap(~ tool) +
  geom_point() +
  labs(x = "Fragment length (1000 nt)",
       y = "Specificity")

ggsave(
  "figures/fungi_specificity.svg",
  width = 6,
  height = 6, standalone=FALSE
)

ggsave(
  "figures/fungi_specificity.png",
  width = 6,
  height = 6
)
```

## Rare events

This will demonstrate given the FPR and TPR, what will precision look like on communities with little phage representation.

```{r, echo = FALSE}
fake_community <- binary_metrics %>%
  select(tool, length, tp, fp) %>%
  rename("tpr" = tp, "fpr" = fp) %>%
  mutate(
    nn = 100,
    even_np = 100,
    mid_np = 10,
    uneven_np = 1,
    even_precision = tpr * even_np / (tpr * even_np + fpr * nn),
    mid_precision = tpr * mid_np / (tpr * mid_np + fpr * nn),
    uneven_precision = tpr * uneven_np / (tpr * uneven_np + fpr * nn)
  ) %>%
  drop_na() %>%
  select(-nn, -even_np, -mid_np, -uneven_np) %>%
  rename("1:1" = even_precision,
         "1:10" = mid_precision,
         "1:100" = uneven_precision) %>%
  pivot_longer(c("1:1",
                 "1:10",
                 "1:100"),
               values_to = "precision",
               names_to = "community")
  

fake_community %>% 
  ggplot(aes(x = length, y = precision, color = community)) +
  facet_wrap(~ tool) +
  geom_point()
```

```{r, echo = FALSE}

precision_curve <- binary_metrics %>% 
  select(tool, length, tp, fp, tn, fn) %>%
  rename("tpr" = tp, "fpr" = fp, "fnr" = fn, "tnr" = tn) %>% 
  merge(tibble(n_pos = c(seq(1, 1.9, by = 0.1), seq(2, 100)),
               n_neg = 100)) %>% 
  mutate(prop_pos = n_pos / n_neg,
         precision = tpr * n_pos / (tpr * n_pos + fpr * n_neg),
         npv = tnr * n_neg / (tnr * n_neg + fnr * n_pos)) %>% 
  drop_na()


precision_curve %>% 
  ggplot(aes(x = n_neg / n_pos, y = precision, color = length)) +
  facet_wrap(~ tool) +
  geom_line(lwd = 1, alpha = 0.75) +
   scale_color_hurwitz("distinguish") +
  labs(x = "Negative:Positive ratio",
       y = "Precision",
       color = "Length (nt)")
```

```{r, echo = FALSE}
nonviral_precision <- precision_curve %>%
  ggplot(aes(
    x = 1 - prop_pos,
    y = precision,
    color = length
  )) +
  facet_wrap( ~ tool) +
  geom_line(lwd = 1, alpha = 0.75) +
  scale_color_hurwitz("distinguish") +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "Non-Viral Percentage of Contigs",
       y = "Precision",
       color = "Length (nt)")

nonviral_precision

ggsave(
  "figures/nonviral_precision.svg",
  width = 6,
  height = 5, standalone=FALSE
)

ggsave(
  "figures/nonviral_precision.png",
  width = 6,
  height = 5
)
```

NPV indicates how well a tool is at excluding non-viral contigs. In this case, it is only considering bacterial contigs.

```{r, echo = FALSE}
precision_curve %>%
  ggplot(aes(
    x = 1 - prop_pos,
    y = npv,
    color = length
  )) +
  facet_wrap( ~ tool) +
  geom_line(lwd = 1, alpha = 0.75) +
  scale_color_hurwitz("distinguish") +
  labs(x = "Non-viral Portion",
       y = "Negative Predictive Value (NPV)",
       color = "Length (nt)")
```

## Big Picture

* mean_cpu: Average CPU time across lengths and organisms (hr)
* speed: Fragments classified per minute CPU time
* speed_quartile: Quartile of classification speed

```{r, echo = FALSE}
big_pic <- bench_df %>%
  filter(tool != "Unlimited Breadsticks") %>%
  group_by(tool) %>%
  summarize(mean_cpu = mean(cpu_time / 60)) %>%
  mutate(
    speed = signif(10000 / (mean_cpu * 60), 3) ,
    speed_quartile = cut(
      speed,
      quantile(speed),
      include.lowest = TRUE,
      labels = FALSE
    )
  )

big_pic <-binary_metrics %>%
  filter(tool != "Unlimited Breadsticks") %>%
  group_by(tool) %>%
  summarize(
    mean_sensitivity = signif(mean(sensitivity), 3),
    mean_precision = signif(mean(precision, na.rm = TRUE), 3)
  ) %>%
  mutate(sensitivity_quartile = cut(
    mean_sensitivity,
    quantile(mean_sensitivity),
    include.lowest = TRUE,
    labels = FALSE
  ),
  precision_quartile = cut(
    mean_precision,
    quantile(mean_precision),
    include.lowest = TRUE,
    labels = FALSE
  )) %>% 
  left_join(big_pic, by = "tool")

big_pic <- fungi_metrics %>% 
  group_by(tool) %>%
  summarize(mean_specificity = signif(mean(specificity), 3)) %>% 
  right_join(big_pic, by = "tool")

big_pic <- caudo_v_non %>% 
  select(tool, length, caudo, sensitivity) %>% 
  pivot_wider(names_from = "caudo", values_from = "sensitivity") %>% 
  rename(other = "Other phages") %>% 
  mutate(retained_sensitivity = other / Caudovirales,
         retained_sensitivity = replace_na(retained_sensitivity, 0),
         retained_sensitivity = case_when(
           retained_sensitivity > 1 ~ 1,
           TRUE ~ retained_sensitivity
         )) %>% 
  group_by(tool) %>% 
  summarize(diverse_phage = signif(mean(retained_sensitivity), 3)) %>% 
  left_join(big_pic, by = "tool")

save(big_pic, file="out_data/big_pic.RData")
```

```{r, echo = FALSE}
color_scale = c("grey", "green")

big_pic_heatmap <- big_pic %>%
  ungroup() %>%
  # select(tool, contains("quartile")) %>%
  select(tool,
         speed,
         mean_sensitivity,
         mean_precision,
         diverse_phage,
         mean_specificity) %>%
  gt() %>%
  cols_label(
    tool = "Tool",
    speed = "Speed",
    mean_precision = "Precision",
    mean_sensitivity = "Sensitivity",
    diverse_phage = "Phage Diversity",
    mean_specificity = "Eukaryote Specificity"
  ) %>%
  data_color(columns = "speed",
             colors = scales::col_numeric(
               palette = color_scale,
               domain = c(min(speed), max(speed))
             )) %>%
  data_color(columns = "mean_sensitivity",
             colors = scales::col_numeric(
               palette = color_scale,
               domain = c(min(mean_sensitivity), max(mean_sensitivity))
             )) %>%
  data_color(columns = "mean_precision",
             colors = scales::col_numeric(
               palette = color_scale,
               domain = c(min(mean_precision), max(mean_precision))
             )) %>%
  data_color(columns = "diverse_phage",
             colors = scales::col_numeric(
               palette = color_scale,
               domain = c(min(diverse_phage), max(diverse_phage))
             )) %>%
  data_color(columns = "mean_specificity",
             colors = scales::col_numeric(
               palette = color_scale,
               domain = c(min(mean_specificity), max(mean_specificity))
             )) %>%
  cols_align(align = "center") %>% 
  cols_width(tool ~ px(110), 
             everything() ~ px(80))


big_pic_heatmap

gtsave(big_pic_heatmap, "figures/bigpic_heatmap.png")

rm(color_scale)
```

